{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages for entire notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlite3, json, re, datetime, math, emoji\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = json.load(open('stopwords.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Messages database using Sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('/Users/Brienna/Library/Messages/chat.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let user identify which handle_id to analyze**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View headings in the Messages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.execute('select * from message')\n",
    "c.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the rowid, text, is_from_me, and datetime columns, placing into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmd1 = 'SELECT ROWID, text, is_from_me, \\\n",
    "        datetime(date + strftime(\\'%s\\',\\'2001-01-01\\'), \\'unixepoch\\') as date_utc \\\n",
    "        FROM message WHERE handle_id=47'\n",
    "c.execute(cmd1)\n",
    "df_msg = pd.DataFrame(c.fetchall(), columns=['id', 'text', 'is_from_me', 'time'])\n",
    "df_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert datatime to something useable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_msg['time'] = [datetime.datetime.strptime(str(t), '%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=-4) for t in df_msg['time']]\n",
    "df_msg['new_date'] = [d.date() for d in df_msg['time']]\n",
    "df_msg['new_time'] = [d.time() for d in df_msg['time']]\n",
    "df_msg['new_hours'] = [d.hour for d in df_msg['time']]\n",
    "df_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long the conversation has been going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = df_msg['new_date'].iloc[0]\n",
    "end = df_msg['new_date'].iloc[-1]\n",
    "print('from ' + str(start) + ' until ' + str(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total messages sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total = len(df_msg)\n",
    "by_me = len(df_msg[df_msg['is_from_me'] == 1])\n",
    "by_himher = total - by_me\n",
    "print('Total: ' + str(total))\n",
    "print('From me: ' + str(by_me))\n",
    "print('From him/her: ' + str(by_himher))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frequencies = {}\n",
    "for message in df_msg['text']:\n",
    "    if message != None:\n",
    "        words = message.split(\" \")\n",
    "        for word in words:\n",
    "            word = re.sub(r'[^\\w\\s]','', word).lower().strip()\n",
    "            if word not in stopwords and word != '':\n",
    "                if word in frequencies:\n",
    "                    frequencies[word] += 1\n",
    "                else:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "frequencies_sorted = sorted(frequencies.items(), key=lambda kv: kv[1])\n",
    "print(frequencies_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most active day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mostCommon(lst):\n",
    "    data = Counter(lst)\n",
    "    return max(lst, key=data.get)\n",
    "\n",
    "most_common_day = mostCommon(list(df_msg['new_date']))\n",
    "print(most_common_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of texts on that day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_on_the_most_active_day = df_msg[df_msg['new_date'] == most_common_day]\n",
    "num_of_texts = len(df_on_the_most_active_day)\n",
    "num_of_texts_from_me = len(df_on_the_most_active_day[df_msg['is_from_me'] == 1])\n",
    "num_of_texts_from_himher = num_of_texts - num_of_texts_from_me\n",
    "\n",
    "print('Total texts sent on ' + str(most_common_day) + ' was ' + str(num_of_texts))\n",
    "print('From me: ' + str(num_of_texts_from_me))\n",
    "print('From him/her: ' + str(num_of_texts_from_himher))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average messages per day that we texted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages_total = 0;\n",
    "distinct_days = 0;\n",
    "last_day_tracked = None;\n",
    "\n",
    "for index, row in df_msg.iterrows():\n",
    "    message = row['text']\n",
    "    if message != None:\n",
    "        messages_total += 1\n",
    "        current_day = row['new_date']\n",
    "        if last_day_tracked != current_day: \n",
    "            distinct_days += 1\n",
    "        last_day_tracked = current_day\n",
    "            \n",
    "print('Sent ' + str(math.floor(messages_total / distinct_days)) + ' messages on average each day.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create radar/spider plot showing average daily activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Format data frames\n",
    "\n",
    "df_24hrs_me = df_msg[df_msg['is_from_me'] == 1]['new_hours']\n",
    "df_24hrs_himher = df_msg[df_msg['is_from_me'] == 0]['new_hours']\n",
    "values_me = df_24hrs_me.value_counts().sort_index().values.flatten().tolist() # IMPORTANT TO SORT HOURS\n",
    "values_himher = df_24hrs_himher.value_counts().sort_index().values.flatten().tolist() # IMPORTANT TO SORT HOURS\n",
    "\n",
    "# We need to repeat the first value to close the circular graph:\n",
    "values_me += values_me[:1]\n",
    "values_himher += values_himher[:1]\n",
    "\n",
    "# Get number of variables\n",
    "categories = set(list(df_msg['new_hours'])[1:]) # set() reduces to distinct values\n",
    "N = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set angle of each axis in the plot (again repeating first value to close the circular graph)\n",
    "angles = [n / float(N) * 2 * math.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize spider plot\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "## If you want the first axis to be on top\n",
    "ax.set_theta_offset(math.pi/2)\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Draw one axe per variable + add labels \n",
    "plt.xticks(angles[:-1], categories, color='grey', size=8);\n",
    "\n",
    "# Draw ylabels\n",
    "ax.set_rlabel_position(0)\n",
    "plt.yticks([1000,2000,3000,4000,5000,6000], [\"1k\", \"2k\", \"3k\",\"4k\",\"5k\",\"6k\"], color='grey', size=8)\n",
    "plt.ylim(0,max(values_me))\n",
    "\n",
    "## ----------- Plot Individual 1 :: me\n",
    "ax.plot(angles, values_me, linewidth=1, linestyle='solid')\n",
    "ax.fill(angles, values_me, 'b', alpha=0.1);\n",
    " \n",
    "## ----------- Plot Individual 2 :: himher\n",
    "ax.plot(angles, values_himher, linewidth=1, linestyle='solid')\n",
    "ax.fill(angles, values_himher, 'r', alpha=0.1)\n",
    "\n",
    "red_patch = mpatches.Patch(color='r', label='Him',alpha=0.1)\n",
    "blue_patch = mpatches.Patch(color='b', label='Me',alpha=0.1)\n",
    "plt.legend(handles=[red_patch, blue_patch],loc='upper right', bbox_to_anchor=(0.1,0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First occurrence of \"I love you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_msg[df_msg['text'].str.contains('i love you', case=False) == True].sort_values(by='time').head(10)\n",
    "\n",
    "\n",
    "# idxmax shows first index value by condition, only necessitates that index is unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative query to avoid cases like \"I love your wordplay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "love_tests = pd.Series(['I love you', 'I love your wordplay']) # I don't really have other instances rn\n",
    "love_tests.str.contains(r'i love you\\b.*', case=False)\n",
    "ilys = df_msg[df_msg['text'].str.contains(r'i love you\\b.*', case=False) == True].sort_values(by='time')\n",
    "print('Said \"I love you\" ' + str(len(ilys)) + ' times')\n",
    "ilys.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The actual texts:\n",
    "print(df_msg.iloc[54171].text)\n",
    "print(df_msg.iloc[54172].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "messages_week = df_msg.groupby(pd.Grouper(key='time', freq='W-MON')).count()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 10)\n",
    "plt.plot(messages_week.text, label='messages', color='pink') # the 200 is where on y axis the arrow points to\n",
    "\n",
    "fig.suptitle('Weekly message overview', fontsize=20)\n",
    "plt.xlabel('Weeks', fontsize=18)\n",
    "plt.ylabel('Messages', fontsize=18)\n",
    "plt.annotate('Ireland', (mdates.date2num(datetime.datetime(2018, 3, 15)), 200), xytext=(-100,0), \n",
    "            textcoords='offset points', size=20,\n",
    "            va='center', ha='center',\n",
    "            arrowprops=dict(arrowstyle=\"->\",\n",
    "                           connectionstyle='arc3, rad=-0.2',\n",
    "                           lw=2),\n",
    "            )\n",
    "plt.annotate('Bri in DC', (mdates.date2num(datetime.datetime(2018, 8, 28)), 205), xytext=(50, -50),\n",
    "            textcoords='offset points', size=20,\n",
    "            va='center', ha='center',\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same analysis as above but with heat map (BROKEN RN) \n",
    "http://nbviewer.jupyter.org/github/home-assistant/home-assistant-notebooks/blob/master/DataExploration-2/DataExploration-2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build data frame for heatmap. This data frame contains dates, days of the week, and frequency of texts on that day. \n",
    "\n",
    "# Need to populate missing dates with zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get dates column\n",
    "dates = df_msg['new_date']\n",
    "# Set counter for dates column  \n",
    "counter = Counter(dates)\n",
    "# Access number of texts on certain date \n",
    "print(counter.get(datetime.date(2016,7,9)))\n",
    "print()\n",
    "\n",
    "def label_day(date):\n",
    "    day_of_week = date.weekday()\n",
    "    switcher = {\n",
    "        0: \"Monday\",\n",
    "        1: \"Tuesday\",\n",
    "        2: \"Wednesday\",\n",
    "        3: \"Thursday\",\n",
    "        4: \"Friday\",\n",
    "        5: \"Saturday\",\n",
    "        6: \"Sunday\"\n",
    "    }\n",
    "    return switcher.get(day_of_week)\n",
    "\n",
    "\n",
    "\n",
    "df_calendar = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n",
    "df_calendar = df_calendar.rename(columns={'index':'date', 0:'frequency'})\n",
    "# Axis 1 is for processing by rows\n",
    "df_calendar['day_of_week'] = df_calendar.apply(lambda row: label_day(row['date']), axis=1)\n",
    "print(df_calendar)\n",
    "num_years = set(list(df_calendar['date'])[1:])\n",
    "\n",
    "# Split data frame into separate data frame for each year\n",
    "year2016 =  pd.date_range('2016/01/01','2016/12/31',freq='H')\n",
    "dt = datetime.datetime(2016,1,3)\n",
    "rows = df_calendar.apply(lambda row: row['date'].year == dt.year, axis=1)\n",
    "year2016data = df_calendar[rows]\n",
    "\n",
    "\n",
    "print(df_calendar)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50,3)) \n",
    "data = df_calendar.pivot(\"day_of_week\", \"date\", \"frequency\")\n",
    "#ax = sns.heatmap(data)\n",
    "\n",
    "year2016datapivoted = year2016data.pivot(\"day_of_week\", \"date\", \"frequency\")\n",
    "ax = sns.heatmap(year2016datapivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if _matplotlib_version >= '1.5':\n",
    "    axisbgc = ax.get_facecolor()\n",
    "else:\n",
    "    axisbgc = ax.get_axis_bgcolor()\n",
    "\n",
    "all_days = pd.date_range('1/15/2014', periods=700, freq='D')\n",
    "days = np.random.choice(all_days, 500)\n",
    "events = pd.Series(np.random.randn(len(days)), index=days)\n",
    "calmap.yearplot(events, year=2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NATURAL LANGUAGE PROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/guiem/my_notebooks/blob/master/anniversary/anniversary.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word cloud (needs stopwords and fixing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fullTexts = \"\"\n",
    "for message in df_msg['text']:\n",
    "    if message != None:\n",
    "        fullTexts += message.lower()\n",
    "\n",
    "def generate_wordcloud(text):\n",
    "    wordcloud = WordCloud(font_path = '/Library/Fonts/Verdana.ttf',\n",
    "                         relative_scaling = 1.0).generate(text)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud(fullTexts)\n",
    "\n",
    "df_with_text = df_msg.text_normalized.dropna()\n",
    "top_1000 = pd.Series(' '.join(df_with_text).split()).value_counts()[:1000]\n",
    "wc = WordCloud(background_color='white')\n",
    "wc.generate_from_frequencies(list(top_1000).to_dict().items())\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to respond to a previous message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Emojis**\n",
    "\n",
    "This code captures emojis that are followed by a space. Otherwise I'll need to substitute regexs for every emoji.\n",
    "\n",
    "Bugs:\n",
    "- Some emojis have two code points \\u\\u, and those are not recognized...? Such as the red heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_msg['text'].str.findall('[^\\w\\s,]')\n",
    "\n",
    "emojis = []\n",
    "for index, row in df_msg.iterrows():\n",
    "    message = row['text']\n",
    "    if message: # some messages are None?\n",
    "        for word in message.split(' '):\n",
    "            for char in word:\n",
    "                if char in emoji.UNICODE_EMOJI:\n",
    "                    emojis.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Counter(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frequent_emojis = Counter(emojis).most_common(15)\n",
    "frequent_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "freqs = []\n",
    "for emoji in frequent_emojis:\n",
    "    labels.append(emoji[0])\n",
    "    freqs.append(emoji[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = pd.Series(freqs).plot(kind='bar', color='pink', width=0.8)\n",
    "ax.set_title('Most Frequently Used Emojis')\n",
    "ax.set_ylabel('Frequency', fontsize=15)\n",
    "ax.set_xlabel('Emojis', fontsize=15)\n",
    "plt.tick_params(\n",
    "    axis='x',\n",
    "    which='both',\n",
    "    bottom=False,\n",
    "    top=False,\n",
    "    labelbottom=False\n",
    ")\n",
    "\n",
    "new_ylim = ax.get_ylim()[1]+30\n",
    "ax.set_ylim((0, new_ylim))\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "# Make labels\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        (rect.get_x() + rect.get_width()/2, height+5),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=30\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To Toggle Scrolling go Cell > Current Outputs > Toggle Scrolling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make stacked chart with my usages and his usages stacked.\n",
    "\n",
    "And do this:\n",
    "https://medium.freecodecamp.org/and-the-most-popular-developer-emoji-is-d660a9687be7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
